---
title: Conditioning and information
slug: /conditioning-and-information/intro
date: 2021-04-12
tags: [
  probability,
  meta-probability,
  sigma-algebras,
  information,
  rats
]
related: [
  /why-so-abstract
]
references: [
  bayes-theorem-wiki
]
---

import { Link } from 'gatsby';

import Cite from '../../src/components/cite';
import Proof from '../../src/components/proof';
import Density from './density';
import ratPath from './rat-path.svg';

In a previous post, <Link to="/why-so-abstract">Why so abstract $(\Omega, \Sigma, \rmP)$?</Link>, I made the following bold claim.

> We use probability to model and analyze the unpredictable outcomes of all sorts of objects $X$ we would otherwise measure in the real world.

Following on this idea, one might argue that _unpredictable_ is a relative concept: what one rat might find unpredictable might be entirely explainable from another's point of view.
In probability, the enlightened rat's knowledge is best explained by identifying a random object $Z$ it has *observed*.
The naive rat will see $Z$ as taking some nontrivial distribution $\mu$ on a measurable space $(O, \calO)$, where the enlightened one will somehow be able to claim it takes some specific value $z \in O$.

## Constructing conditional models

<p>{ String.raw`
  From the persepctive of applied probability, we can easily construct models that do this.
  Namely, for each possible observation $z \in O$, we declare a probability space $(\Omega', \Sigma', \rmP_z)$ such that the family $(\rmP_z)_{z\in O}$ yields $\calO/\calB_{[0,1]}$-measurability for the maps
  $$ z \mapsto \rmP_zA, ~A \in \Sigma'. $$
  This way, each of these maps is integrable, so we now have the following map $\Sigma' \times \calO \mapsto [0,1]$.
  $$ A \times B \mapsto \int_B \rmP_zA \,\mu({\rm d}z) $$
  This map uniquely extends to a probability measure $\rmP$ on the product space 
  $$ (\Omega, \Sigma) \coloneqq (\Omega'\times O, \Sigma' \otimes \calO). $$
  From the perspective of Monte Carlo, a sample $(\omega', z) \sim \rmP$ is equivalent to sequentially sampling $z \sim \mu$ and $\omega' \sim \rmP_z$.
  This would be the perspective of the naive rat, while the enlightened one can bypass immediately to $\omega' \sim \rmP_z$ for the observed $z \in O$. 
  This indicates to us that we would then define $Z: \Omega \rightarrow O$ as the component map $Z(\omega, z) = z$.
  This way, each $B \in \calO$ of positive $\mu$-measure is such that
  $$ \rmP( A | Z \in B) = \frac{\rmP(A, Z \in B)}{\rmP(Z \in B)} = \frac{\int_B \rmP_zA \,\mu({\rm d}z)}{\mu B}. $$
  Provided $(O, \calO)$ is a Borel algebra induced by a nice enough topology, it then makes sense that decreasing $B \downarrow \{z\}$ would limit the above expression to 
  $$ \rmP(A | Z = z) = P_zA. $$
  This confirms all of the heuristics we desired.
`}
</p>

<p>{ String.raw`
  We can easily create such models when we are in finite dimensions and have absolute continuity with respect to the Lebesgue measure.
  Namely, we select our favorite continuous functions $p: \bbR^n \rightarrow \bbR$ and $p(\cdot|\cdot): \bbR^m \times \bbR^n \rightarrow \bbR$ such that
  $$\begin{gathered} \int_{\bbR^n} p(z) \rmd z = 1, \\ \int_{\bbR^m} p(x|z) \rmd x = 1, ~ z \in \bbR^n, \end{gathered}$$
  and observe that we may construct our family $(\rmP_z)_{z \in \bbR^n}$ like so.
  $$ P_z = A \mapsto \int_A p(x|z) \rmd x $$
  From here, the joint probability measure $\rmP$ on $\bbR^m \times \bbR^n$ can be equipped with random variables $X(x, z) = x$ and $Z(x, z) = z$ so that $Z \sim p$ and $X \sim p(\cdot|Z)$.
`}</p>

## Extracting conditional models

<p>{ String.raw`
  Often times, we are concerned with going the other direction.
  Suppose we are already provided with some probability space $(\Omega, \Sigma, \rmP)$, a measurable space $(O, \calO)$, and $Z: \Omega \rightarrow O$ with distribution $\mu$.
  How do we declare the existence of a measure $\rmP(\cdot|Z=z)$ on $(\Omega, \Sigma)$ for each $z \in O$, such that we have the desired factoring?
  $$ \rmP(A, Z \in B) = \int_B \rmP(A|Z=z) \mu({\rm d}z) $$
  This is important to understand for probabilists, as it allows them to declare the existence of conditional distributions for complicated objects like stochastic processes.
  For example, we may have some process $W: \Omega \rightarrow (\bbR^2)^{[0,T]}$ of which we have observed the trajectory $Z = W|_{[0,t]}$ up to a time $t < T$ and are concerned with seeing how the terminal location $W_T$ distributes.
`}</p>

<div className="card" style={{ margin: '1em auto', maxWidth: '800px' }}>
  <div className="card-image">
    <img 
      src={ ratPath } 
      alt="complicated-rat-path" 
      style={{ maxHeight: '300px', margin: '1em auto', display: 'block' }}/>
  </div>
  <div className="card-content">{ String.raw`
      Sam will walk a random path $W$.
      At time $t < T$, Sam reflects on his path $Z = W|_{[0,t]}$ he made.
      He observes this path is $z \in (\bbR^2)^{[0,t]}$; how does his final position $W_T$ now distribute?
      $$ \rmP( W_T \in A | Z = z ) = \; ??? $$
    `}
  </div>
</div>

If we impose the very strong assumptions that $W$ is Markov and that the joint distribution of $(W_T, W_t)$ is absolutely continuous with respect to the Lebesgue measure, then the answer requires a simple use of <Cite bibKey="bayes-theorem-wiki">Bayes' theorem</Cite>.
Namely, if $p$ is the density of this distribution,

$$ \rmP(W_T \in A, W_t \in B) = \int_B\int_A p(x,y) \rmd x \rmd y, $$

then we may *define* a conditional distribution $\rmP(W_T \in \cdot | W_t = y)$ by a density function $p(\cdot|y)$ with respect to Lebesgue.

$$ p(x|y) = \frac{p(x, y)}{\int_{\bbR^2} p(x', y) {\rm d}x'} $$

<div className="card"
  style={{
    display: 'block',
    margin: '1em auto',
    maxWidth: '800px', 
  }}
  >
  <Density className="card-image" />
  <div className="card-content">{ String.raw`
    The value $p(x, y)$ represents a likelihood of $(W_T, W_t)$ at the location $(x, y)$.
    The slice $x \mapsto p(x, y)$ of the graph for some fixed $y$ should have the right likelihoods, but we must normalize to get $p(x|y)$.
  `}</div>
</div>



Defining the conditional distribution like this will satisfy the factoring condition we require.

<Proof>{ String.raw`
  First, we acknowledge that the marginal distribution for $W_t$ is the following measure $\mu$.
  $$\begin{aligned} 
    \mu(\rmd y) &= \rmP(W_t \in \rmd y) \\
    &= \rmP(W_T \in \bbR^2, W_t \in \rmd y) \\
    &= \Big(\int_{\bbR^2} p(x, y) \rmd x \Big) \rmd y
  \end{aligned}$$
  Now, we have our desired factoring.
  $$\begin{aligned}
    &\int_B \rmP(W_T \in A | W_t = y) \mu(\rmd y) \\
    &\quad= \int_B \Big(\int_A p(x|y) \rmd x \Big) \Big(\int_{\bbR^2} p(x, y) \rmd x \Big) \rmd y \\
    &\quad= \int_B \Bigg(\int_A \frac{p(x, y)}{\int_{\bbR^2} p(x', y) \rmd x'} \rmd x \Bigg) \Big(\int_{\bbR^2} p(x, y) \rmd x \Big) \rmd y \\
    &\quad= \int_B \int_A p(x, y) \rmd x \rmd y \\
    &\quad= \rmP(W_T \in A, W_t \in B)
  \end{aligned}$$
`}</Proof>

<p>{ String.raw`
  The Markovian assumption then gives us he property we need.
  $$ \rmP(W_T \in A | Z=z) = \rmP\big(W_T \in A | W_t = z(t)\big) = \frac{\int_A p\big(x, z(t)\big) \rmd x}{\int_{\bbR^2} p\big(x, z(t)\big) \rmd x}$$
`}</p>

Without somehow knowing *a priori* that we are dealing with a Markov process, how do we understand a conditional distribution $\rmP(W_T \in \rmd x|Z=z)$, or moreover a conditional measure $\rmP(\rmd \omega|Z=z)$?
If we define $\rmP(\rmd \omega|Z=z)$ by our factoring condition,

$$ \rmP(A, Z \in B) = \int_B \rmP(A|Z=z) \mu(\rmd z) = \int_B \rmP(A|Z=z) \rmP(Z\in\rmd z) $$

how can we know that it in fact exists?
In the following posts, I intend to show how this is done from a purely measure theoretic point of view.
The work is broken into multiple sections.

